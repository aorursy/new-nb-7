# standard imports

import time

from IPython.display import display

import numpy as np

import pandas as pd



# pytorch imports

import torch

import torch.nn as nn

import torch.utils.data



# imports for preprocessing the questions

from keras.preprocessing.text import Tokenizer

from keras.preprocessing.sequence import pad_sequences



# cross validation and metrics

from sklearn.model_selection import StratifiedKFold

from sklearn.metrics import f1_score



# progress bars

from tqdm import tqdm

tqdm.pandas()
train_df = pd.read_csv("../input/train.csv")

test_df = pd.read_csv("../input/test.csv")

print('Train data dimension: ', train_df.shape)

display(train_df.head())

print('Test data dimension: ', test_df.shape)

display(test_df.head())
def seed_torch(seed=1234):

    np.random.seed(seed)

    torch.manual_seed(seed)

    torch.cuda.manual_seed(seed)

    torch.backends.cudnn.deterministic = True
def threshold_search(y_true, y_proba):

    best_threshold = 0

    best_score = 0

    for threshold in tqdm([i * 0.01 for i in range(100)]):

        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)

        if score > best_score:

            best_threshold = threshold

            best_score = score

    search_result = {'threshold': best_threshold, 'f1': best_score}

    return search_result
def sigmoid(x):

    return 1 / (1 + np.exp(-x))
embed_size = 300 # how big is each word vector

max_features = 75000 # how many unique words to use (i.e num rows in embedding vector)

maxlen = 50 # max number of words in a question to use
puncts = [',', '.', '"', ':', ')', '(', '-', '!', '?', '|', ';', "'", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\', '•',  '~', '@', '£', 

 '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', 

 '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', 

 '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', 

 '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]

def clean_text(x):

    x = str(x)

    for punct in puncts:

        x = x.replace(punct, f' {punct} ')

    return x
train_df["question_text"] = train_df["question_text"].str.lower()

test_df["question_text"] = test_df["question_text"].str.lower()



train_df["question_text"] = train_df["question_text"].apply(lambda x: clean_text(x))

test_df["question_text"] = test_df["question_text"].apply(lambda x: clean_text(x))



# fill up the missing values

x_train = train_df["question_text"].fillna("_##_").values

x_test = test_df["question_text"].fillna("_##_").values



# Tokenize the sentences

tokenizer = Tokenizer(num_words=max_features)

tokenizer.fit_on_texts(list(x_train))

x_train = tokenizer.texts_to_sequences(x_train)

x_test = tokenizer.texts_to_sequences(x_test)



# Pad the sentences 

x_train = pad_sequences(x_train, maxlen=maxlen)

x_test = pad_sequences(x_test, maxlen=maxlen)



# Get the target values

y_train = train_df['target'].values
def load_glove(word_index):

    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'

    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]

    embeddings_index = dict(get_coefs(*o.split(" ")) for o in open(EMBEDDING_FILE))

    

    all_embs = np.stack(embeddings_index.values())

    emb_mean,emb_std = -0.005838499,0.48782197

    embed_size = all_embs.shape[1]



    # word_index = tokenizer.word_index

    nb_words = min(max_features, len(word_index))

    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))

    for word, i in word_index.items():

        if i >= max_features: continue

        embedding_vector = embeddings_index.get(word)

        if embedding_vector is not None: embedding_matrix[i] = embedding_vector

            

    return embedding_matrix 

    

def load_fasttext(word_index):    

    EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'

    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')

    embeddings_index = dict(get_coefs(*o.split(" ")) for o in open(EMBEDDING_FILE) if len(o)>100)



    all_embs = np.stack(embeddings_index.values())

    emb_mean,emb_std = all_embs.mean(), all_embs.std()

    embed_size = all_embs.shape[1]



    # word_index = tokenizer.word_index

    nb_words = min(max_features, len(word_index))

    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))

    for word, i in word_index.items():

        if i >= max_features: continue

        embedding_vector = embeddings_index.get(word)

        if embedding_vector is not None: embedding_matrix[i] = embedding_vector



    return embedding_matrix



def load_para(word_index):

    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'

    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')

    embeddings_index = dict(get_coefs(*o.split(" ")) for o in open(EMBEDDING_FILE, encoding="utf8", errors='ignore') if len(o)>100)



    all_embs = np.stack(embeddings_index.values())

    emb_mean,emb_std = -0.0053247833,0.49346462

    embed_size = all_embs.shape[1]



    # word_index = tokenizer.word_index

    nb_words = min(max_features, len(word_index))

    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))

    for word, i in word_index.items():

        if i >= max_features: continue

        embedding_vector = embeddings_index.get(word)

        if embedding_vector is not None: embedding_matrix[i] = embedding_vector

    

    return embedding_matrix
glove_embeddings = load_glove(tokenizer.word_index)

paragram_embeddings = load_para(tokenizer.word_index)



embedding_matrix = np.mean([glove_embeddings, paragram_embeddings], axis=0)

np.shape(embedding_matrix)
splits = list(StratifiedKFold(n_splits=4, shuffle=True, random_state=10).split(x_train, y_train))
class Attention(nn.Module):

    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):

        super(Attention, self).__init__(**kwargs)

        

        self.supports_masking = True



        self.bias = bias

        self.feature_dim = feature_dim

        self.step_dim = step_dim

        self.features_dim = 0

        

        weight = torch.zeros(feature_dim, 1)

        nn.init.xavier_uniform_(weight)

        self.weight = nn.Parameter(weight)

        

        if bias:

            self.b = nn.Parameter(torch.zeros(step_dim))

        

    def forward(self, x, mask=None):

        feature_dim = self.feature_dim

        step_dim = self.step_dim



        eij = torch.mm(

            x.contiguous().view(-1, feature_dim), 

            self.weight

        ).view(-1, step_dim)

        

        if self.bias:

            eij = eij + self.b

            

        eij = torch.tanh(eij)

        a = torch.exp(eij)

        

        if mask is not None:

            a = a * mask



        a = a / torch.sum(a, 1, keepdim=True) + 1e-10



        weighted_input = x * torch.unsqueeze(a, -1)

        return torch.sum(weighted_input, 1)
128*4
batch_size = 512 # how many samples to process at once

n_epochs = 5 # how many times to iterate over all samples
class NeuralNet(nn.Module):

    def __init__(self):

        super(NeuralNet, self).__init__()

        

        hidden_size = 128

        

        self.embedding = nn.Embedding(max_features, embed_size)

        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))

        self.embedding.weight.requires_grad = False

        

        self.embedding_dropout = nn.Dropout2d(0.1)

        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)

        self.gru = nn.GRU(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)

        

        self.lstm_attention = Attention(hidden_size * 2, maxlen)

        self.gru_attention = Attention(hidden_size * 2, maxlen)

        

        self.linear = nn.Linear(1024, 16)

        self.relu = nn.ReLU()

        self.dropout = nn.Dropout(0.1)

        self.out = nn.Linear(16, 1)

    

    def forward(self, x):

        h_embedding = self.embedding(x)

        h_embedding = torch.squeeze(

            self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))

        

        h_lstm, _ = self.lstm(h_embedding)

        h_gru, _ = self.gru(h_lstm)

        

        h_lstm_atten = self.lstm_attention(h_lstm)

        h_gru_atten = self.gru_attention(h_gru)

        

        # global average pooling

        avg_pool = torch.mean(h_gru, 1)

        # global max pooling

        max_pool, _ = torch.max(h_gru, 1)

        

        conc = torch.cat((h_lstm_atten, h_gru_atten, avg_pool, max_pool), 1)

        conc = self.relu(self.linear(conc))

        conc = self.dropout(conc)

        out = self.out(conc)

        

        return out
# matrix for the out-of-fold predictions

train_preds = np.zeros((len(train_df)))

# matrix for the predictions on the test set

test_preds = np.zeros((len(test_df)))



# always call this before training for deterministic results

seed_torch()



x_test_cuda = torch.tensor(x_test, dtype=torch.long).cuda()

test = torch.utils.data.TensorDataset(x_test_cuda)

test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)



for i, (train_idx, valid_idx) in enumerate(splits):    

    # split data in train / validation according to the KFold indeces

    # also, convert them to a torch tensor and store them on the GPU (done with .cuda())

    x_train_fold = torch.tensor(x_train[train_idx], dtype=torch.long).cuda()

    y_train_fold = torch.tensor(y_train[train_idx, np.newaxis], dtype=torch.float32).cuda()

    x_val_fold = torch.tensor(x_train[valid_idx], dtype=torch.long).cuda()

    y_val_fold = torch.tensor(y_train[valid_idx, np.newaxis], dtype=torch.float32).cuda()

    

    model = NeuralNet()

    # make sure everything in the model is running on the GPU

    model.cuda()



    # define binary cross entropy loss

    # note that the model returns logit to take advantage of the log-sum-exp trick 

    # for numerical stability in the loss

    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')

    optimizer = torch.optim.Adam(model.parameters())



    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)

    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)

    

    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)

    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)

    

    print(f'Fold {i + 1}')

    

    for epoch in range(n_epochs):

        # set train mode of the model. This enables operations which are only applied during training like dropout

        start_time = time.time()

        model.train()

        avg_loss = 0.  

        for x_batch, y_batch in tqdm(train_loader, disable=True):

            # Forward pass: compute predicted y by passing x to the model.

            y_pred = model(x_batch)



            # Compute and print loss.

            loss = loss_fn(y_pred, y_batch)



            # Before the backward pass, use the optimizer object to zero all of the

            # gradients for the Tensors it will update (which are the learnable weights

            # of the model)

            optimizer.zero_grad()



            # Backward pass: compute gradient of the loss with respect to model parameters

            loss.backward()



            # Calling the step function on an Optimizer makes an update to its parameters

            optimizer.step()

            avg_loss += loss.item() / len(train_loader)

            

        # set evaluation mode of the model. This disabled operations which are only applied during training like dropout

        model.eval()

        

        # predict all the samples in y_val_fold batch per batch

        valid_preds_fold = np.zeros((x_val_fold.size(0)))

        test_preds_fold = np.zeros((len(test_df)))

        

        avg_val_loss = 0.

        for i, (x_batch, y_batch) in enumerate(valid_loader):

            y_pred = model(x_batch).detach()

            

            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)

            valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]

        

        elapsed_time = time.time() - start_time 

        print('Epoch {}/{} \t loss={:.4f} \t val_loss={:.4f} \t time={:.2f}s'.format(

            epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))

        

    # predict all samples in the test set batch per batch

    for i, (x_batch,) in enumerate(test_loader):

        y_pred = model(x_batch).detach()



        test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]



    train_preds[valid_idx] = valid_preds_fold

    test_preds += test_preds_fold / len(splits)
search_result = threshold_search(y_train, train_preds)

search_result
submission = test_df[['qid']].copy()

submission['prediction'] = test_preds > search_result['threshold']

submission.to_csv('submission.csv', index=False)